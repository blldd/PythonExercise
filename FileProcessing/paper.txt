Auto-encoder based Normalized Network Embedding Method
for Top-K Item Recommendation
Abstract
Data sparsity and diversity of recommendation are two ma-
jor problems in recommendation system. The way to fully
exploit the hidden features among users and items has at-
tracted extensive research. The latent factor model (LFM)
achieves good performance in Netflix Prize, which devoted
to exploit latent factor. However, the problem of data sparsity
in practical and expensive matrix decomposition operations
make it difficult to deal with large scale networks. Network
representation learning algorithm attracts much interest re-
cently for ability to describe high-dimensional complex net-
works as low-dimensional dense vectors, making it possible
to work with machine learning algorithm. Deep neural net-
work has shown a powerful ability in capturing useful infor-
mation of complex network. Therefore, we propose a method
to make top-k item recommendation through a normalized
network embedding method based on auto-encoder. We ana-
lyzetheperformanceofseveraleffectivealgorithmsonDBLP
and MovieLens1M datasets. Experimental results show that
the proposed framework greatly improves the diversity and
novelty compared with the existing algorithms, while the F1
score varies little.
Introduction
Recommendation systems are widely used in industry. We
have music applications, news applications and shopping
applications in our mobile phone, which are supported by
recommendation systems with no doubt. However, there are
problems such as sparse data and lack of diversity in the rec-
ommendation system. The study found that we do not make
full use of the hidden features of the data (Zhang, Yao, and
Sun 2017). Therefore, how to fully exploit the implicit fea-
tures among users and items has become the main goal of
recommendation system.
The latent factor model (LFM) achieves good perfor-
mance in Netflix Prize, which decomposes the rating matrix
R into two low-dimensional matrices, trying to learn the im-
plicit features of the data (Shen and Jin 2012). However, the
problem of data sparsity in practical and expensive matrix
decomposition operations make it difficult to deal with large
scale networks.
Network representation is the main representation for de-
scribing the relationship between objects. The key is how
Copyright c ? 2019, Association for the Advancement of Artificial
Intelligence (www.aaai.org). All rights reserved.
to represent the features contained in the network effec-
tively. Since the network representation learning algorithm
can describe high-dimensional complex networks as low-
dimensional dense vectors, making it possible to combine
complex tasks with machine learning algorithm effectively
(Cui et al. 2017; Wang et al. 2017; 2014). In short, apply
network representation learning algorithms to recommenda-
tion systems plays an important role in academic value and
application value (Cunchao et al. 2017). Simultaneously, the
network embedding method is more effective than matrix
decomposition in feature learning (Goyal and Ferrara 2018).
In addition, the deep neural network has shown a revolu-
tionary advances in image(Schmidhuber 2012; Dan, Meier,
and Schmidhuber 2012; Xu et al. 2014), speech(Hinton et
al. 2012; Graves, Mohamed, and Hinton 2013) and natural
language processing(Collobert and Weston 2008), which at-
tracts researchers to apply it in capturing useful information
of complex network(Zhang, Yao, and Sun 2017). (Li and
She 2017) show that the auto-encoder performs robust on
extracting the implicit relationships between items and users
collaboratively when processing the network representation.
In order to solve the problem of data sparsity in rating
matrix of recommendation system, we transform the rating
matrix into user adjacency matrix and item adjacent matrix,
which turns out to increase the data density effectively. Re-
cent research on Taobao recommendation scenario shows
that the representation of complex networks by graph em-
bedding can solve the problem of data sparse and cold start
to a certain extent (Zhu et al. 2018). Above all, we propose
to choose graph embedding method with the help of auto-
encoder to train our model.
On the process of dealing with diversity and novelty, we
find that hot items usually have a high weight in the net-
work, which leads to the large value of the final vector repre-
sentation. As a result, the items recommended for each user
are much the same (almost hot items only), which leaves a
negative impact on users permanently even the accuracy and
F1 metric of the experiment are good. Correspondingly, we
found that the hot goods can be punished to a certain extent
through the normalized vector representation. Results show
that the prediction of the hot goods moves down in ranking,
which makes ordinary items recommended in the top-k list,
leading to a more diverse results.
In short, the contributions of this paper are listed as fol-
lows:
? We propose an Auto-encoder based Normalized Network
Embedding method for Top-K Recommendation, namely
ANNER, which combines the advantages of auto-encoder
and network embedding algorithms.
? We choose KL-divergence to extract explicit features and
auto-encoder to extract the first-order and higher-order
proximity of implicit features so as to preserve the net-
work structure more comprehensively.
? We suggest to punish the popular items with the help of
L2 norm in the process of network embedding, which
turns out to be a good way to improve variety.
? We have carried on the contrast experiments on the pop-
ular datasets MovieLens and DBLP. Compared with the
previous effective recommendation algorithms, the diver-
sity and novelty score have been greatly improved in our
method while the F1 score varies little.
Related Work
Recommendation Systems
The task of recommendation system is divided into rat-
ing prediction and ranking prediction(Zhang, Yao, and Sun
2017). Rating prediction task is to predict vacant scores in
the rating matrix(Chen et al. 2014). Ranking prediction gen-
erates a list of items (top-k recommendation) that each user
is most likely to consume. Recommendation system mod-
els are mainly based on collaborative filtering, content-based
and hybrid recommendation systems(Felfernig et al. 2005).
The early work is mainly based on collaborative filtering
(CF) method to use the user＊s historical interactive informa-
tion to recommend, including the traditional neighborhood
based algorithm (ItemKNN, UserKNN)(Pan, Dolog, and Xu
2012), latent factor based algorithm(NMF, SVD)(Luo et al.
2014; 2016; Brand 2003) and graph based algorithm(Huang,
Chung, and Chen 2014). Many recent efforts devoted to
use deep learning models (such as CNN, RNN, RBM and
Auto-encoder) to learn multi-layer representations of data to
achieve better results(Zhang, Yao, and Sun 2017).
Network Embedding
As an important form of data representation, graph appears
inawiderangeofreal-worldscenarios.Effectivegraphanal-
ysis enables users to have a better understanding of the fea-
tures behind the data. Network embedding maps graph data
to a low dimensional space, which preserves the structure in-
formation of graphs and the properties of graphs to the max-
imum extent(Cui et al. 2017). Previous studies have shown
that network embedding has a wide range of applications
in node classification, recommendation systems, link pre-
diction, community discovery and visualization(Cai, Zheng,
and Chang 2017).
Network representation learning methods are mainly
based on matrix eigenvector computation, neural network
and matrix decomposition (Cunchao et al. 2017). Recent
methods are mainly based on random walk and deep neu-
ral network(Cai, Zheng, and Chang 2017). Word2Vec im-
proves the efficiency of word representation significantly by
transforming sparse discrete and high-dimensional vectors
intodensecontinuousandlow-dimensionalvectors(Mikolov
et al. 2013). To make analogy with Word2Vec, the random
walk based model (Perozzi, Al-Rfou, and Skiena 2014) gen-
erates random paths on the network. By regarding a node
as a word, we can treat a random path as a sentence, and
the neighborhood of a node can be identified by the co-
occurrence rate in Word2Vec. At the same time, (Yang et
al. 2015) proved that the DeepWalk algorithm is actually
equivalent to the matrix decomposition of a particular ad-
jacent matrix. Then, by learning the first and second order
relations in the network, LINE further mine the information
of the network(Tang et al. 2015). SDNE starts to learn the
higher order relations of the network based on auto-encoder,
which is supposed as to preserve the overall structure of the
network(Wang, Cui, and Zhu 2016), and so on. Recently,
BiNE proposed a network embedding model for bipartite
networks, which is the first work in bipartite network em-
bedding(Gao et al. 2018).
ANNER Framework
The goal of the recommendation system is to recommend
the customized item sequence to each user, so as to provide a
betteruserexperiencetousersasfaraspossible.Thissection
we will describe our framework in detail.
Table 1: Notations and Terms
Symbol Definition
U = {u i } |U|
i=1 ,V = {v j }
|V |
j=1
user set, item set
E ﹋ U ? V inter-set edges
G = (U,V,E) network
R = [r ij ] rating matrix
? ↙
u i ,
? ↙
v j embedding vectors
U = [ ? ↙ u i ], V = [ ? ↙ v j ] embedding matrix
R U , R V adjacent matrix of U, V
X = {x i } n
i=1 ,
?
X = {? x i } n
i=1
input, reconstructed
Y
(l)
= {y (l)
i
} n
i=1
lth layer embeddings
W (l) ,
?
W (l) lth layer weights
b (l) , ? b (l) lth layer biases
牟 = {W (l) ,
?
W (l) ,b (l) , ? b (l) } AE parameters
Problem Definition
Problem definition. The goal of the recommendation sys-
tem is to recommend a list of items most likely to be con-
sumed for each user, in a preference decrease order.
Input. The network G = (U,V,E); Weight matrix of the
network R; Embeddding dimension d.
Output. Predict a sequence of favorite items C i for each
user u i .
Notations. Notations and terms are listed in Table 1.
Framework
Based on the above analysis, in order to preserve the whole
network well, we propose a new framework Auto-encoder
based Normalized Network Embedding method for Top-K
Item Recommendation (ANNER). With KL-divergence to
capture the explicit features and auto-encoder to mine the
implicit features of the network effectively, which can be
observed in Figure 1 and Figure 5.
Figure 1: Flow Diagram of the ANNER Framework.
Explicit Relations
As shown in Figure 2, the edges between nodes u i and v j
are defined as the explicit relationship in the recommenda-
tion system scenario. Inspired by the first-order proximity
in LINE (Tang et al. 2015) , we establish explicit relations
by considering the local proximity between two connected
nodes. The joint probability between nodes u i and v j can be
defined as follows:
P(i,j) =
r ij
P
e ij ﹋E r ij
(1)
where r ij is the weight of edge e ij .
Inspired by Word2Vec (Mikolov et al. 2013), we simulate
the interaction between two entities by inner product to es-
timate the local proximity between two nodes in embedding
space. The interactive value is transformed into probabilistic
space by sigmoid function.
?
P(i,j) =
1
1 + exp(? ? ↙ u i T ? ↙ v j )
(2)
where
? ↙
u i and
? ↙
v j are the embedding vectors of nodes u i
and v j respectively.
With the empirical distribution of the co-occurrence prob-
ability between nodes and the reconstructed distribution, we
can learn the embedding vector by minimizing the differ-
ence. We choose the Kl-divergence as the difference mea-
sure between their distributions. The loss function is defined
as follows:
minimize O KL = KL(P|| ? P)
=≦ ?
X
e ij ﹋E
r ij log ? P(i,j)
(3)
By minimizing the loss function, two connected nodes in
the original network will be close to each other in the em-
bedding space, thus maintaining local proximity.
Figure 2: Explicit relations of the user feedback on items.
Implicit Relations
We define the implicit relationship as the relationship be-
tween the same type nodes, that is, the relationship between
the users, or the relationship between the items. We can see
from Figure 2 that there is an explicit relationship(user＊s rat-
ing of item) between users and items in the recommendation
system. However, the relationship between the users doesn＊t
exist explicitly. As shown in Figure 3, we can obtain the user
adjacent matrix by calculating the similarity between users,
items are the same.
Figure 3: Adjacency Matrix calculated by the inner product.
The left part is a rating matrix of 4 users and 6 items. The
right part are the user adjacent matrix and item adjacent ma-
trix.
In order to fully mine the local and global information of
adjacent matrices, and inspired by LINE (Tang et al. 2015),
we model the first and second order relations of adjacent
matrices, as shown in Figure 4.
Figure 4: Nodes 1 and 3 have a direct connection called first-
order relations; nodes 1 and 2 have multiple common neigh-
bors, which is called second-order relations
Since nodes 1 and 3 have a direct connection, they should
be placed closer in low-dimensional spaces, which is called
first-order relations; nodes 1 and 2 have multiple common
neighbors even they don＊t have an explicit connection, so
they should be placed closer in low-dimensional spaces,
which is called second-order relations.
Second Order Relations The idea of second-order rela-
tions is that two nodes with common neighbors tend to be
closer in embedding space, so the global structure of the net-
work can be well preserved. Inspired by SDNE(Wang, Cui,
and Zhu 2016), we can implement the auto-encoder algo-
rithm with the idea of auto-encoder unsupervised learning.
As shown in Figure 5, both the encoder and decoder parts
contain multilayer nonlinear function, the encoder maps the
rawdatatothelow-dimensionalspace,andthedecodermap-
ping the low-dimensional space representation to the recon-
structed space. In this way, the network is more and more ro-
bustbyminimizingthedifferencebetweentheoriginalspace
and the reconstructed space, so that the low-dimensional
representation of each node can be obtained. For example,
given x i the representation of each layer of the encoder is as
follows:
y (1)
i
= 考(W (1) x i + b (1) ) (4)
y (k)
i
= 考(W (k) y (k?1)
i
+ b (k) ),k = 2,...,K (5)
where y (K)
i
is a low-dimensional representation, and then
decoder can perform an inverse operation to get the recon-
structed ? x, so that the whole auto-coder can be trained by
minimizing the loss functions:
L =
n
X
i=1
k? x i ? x i k 2
2
(6)
On the one hand, the adjacency matrix is not a sparse ma-
trix (The number of zero element in sparse matrix is much
larger than the non-zero element, so we need to add more
weight to non-zero elements to make the training more ef-
fective), which can be seen in Table 2. On the other hand, the
similarity between users are different. So there is no need to
append a bias here. To have a better visualization of weights
in adjacency matrix, we have generated the distribution of
adjacency matrix weights in Figure 6. Since the long tail ef-
fect exists, we only show the middle part for better analysis.
First Order Relations Inspired by laplacian eigen-
maps(LE) (Belkin and Niyogi 2014), we define the follow-
ing loss functions to keep them close to each other in em-
bedding space to preserve the local structure of the network.
L 1st =
n
X
i,j=1
s i,j
?y
(K)
i
? y (K)
j
?
2
2
=
n
X
i,j=1
s i,j ky i ? y j k 2
2
(7)
We can see that by adding a penalty coefficient s ij to the
loss function to make the related nodes closer in the low
dimensional space so as to preserve the local structure of the
network.
Combining the first order relation and the second order
relation, we define the joint optimal loss function as follows:
L ae x = L 2nd + 汐L 1st + 汕L reg
=
? ? X ? X
?
2
F
+ 汐
n
X
i,j=1
s i,j ky i ? y j k 2
2
+ 汕L reg
(8)
L reg =
1
2
K
X
k=1
(
?W
(k) ?
2
F
+
? ? W
(k) ?
2
F
) (9)
Optimization
For the explicit relation, edge e ij ﹋ E, we use Stochastic
Gradient Ascent(SGA) to update the embedding vectors
? ↙
u i
and
? ↙
v j , so as to maximize KL-divergence O KL . The updates
to
? ↙
u i and
? ↙
v j are as follows:
? ↙
u i =
? ↙
u i + 竹{污w ij [1 ? 考( ? ↙ u i T ? ↙ v j )] ﹞
? ↙
v j } (10)
? ↙
v j =
? ↙
v j + 竹{污w ij [1 ? 考( ? ↙ u i T ? ↙ v j )] ﹞
? ↙
u i } (11)
For implicit relations, we want to minimize loss function
L ae user and L ae item , take L ae user as an example, the pa-
tial derivative is shown below:
?L ae user
? ? W (k)
=
?L 2nd
? ? W (k)
+
?L reg
? ? W (k)
(12)
?L ae user
?W (k)
=
?L 2nd
?W (k)
+
?L 1st
?W (k)
+
?L reg
?W (k)
k = 1,...,K
(13)
Firstly,
?L 2nd
? ? W (K)
can be rephrased as follows:
?L 2nd
? ? W (K)
=
?L 2nd
? ? X
﹞
? ? X
? ? W (K)
(14)
and
?L 2nd
? ? X
can be rephrased as follows:
?L 2nd
? ? X
= 2( ? X ? X) (15)
For the second term, since
?
X = 考( ? Y
(K?1) ?
W (K) + ? b (K) ),
so we can get
?L 2nd
? ? W (K) , and based on hack-propagation,
we can iteratively obtain
?L 2nd
? ? W (k) ,k = 1,...,K ? 1 and
?L 2nd
?W (k) ,k = 1,...,K.
Secondly, we continue to calculate the partial derivative
of
?L 1st
?W (k) . The loss function of L 1st
can be rephrased as fol-
lows:
L 1st =
n
X
i,j=1
s i,j ky i ? y j k 2
2
= 2tr(Y
T LY )
(16)
whereL = D?S,D ﹋ R n?n isadiagonalmatrix,D i,i =
P
j s i,j .
Then the calculation of
?L 1st
?W (K)
can be rephrased as fol-
lows:
?L 1st
?W (K)
=
?L 1st
?Y
﹞
?Y
?W (K)
(17)
Figure 5: ANNER Framework, the implicit part include two auto-encoder to train on user adjacent matrix and item adjacent
matrixseperately,theexplicitpartfinetunetheembeddingbyKL-divergence,thenormalizepartaredesignedtofixthediversity
problem.
Figure 6: Distributions of weights of adjacent matrix of
DBLP dataset.
We can get
?Y
?W (K) , since Y = 考(Y
(K?1) W (K)
+ b (K) ).
And
?L 1st
?Y
can be rephrased as follows:
?L 1st
?Y
= 2(L + L T ) ﹞ Y (18)
Similar to L 2nd , we can get the calculation of partial
derivative of L 1st .
Since we have calculated the partial derivatives of the pa-
rameters, we can optimize our model using stochastic gradi-
ent descent. Empirically, we pretrain our parameters first in
order to find a good region of parameter space. To sum up,
our whole framework works as Algorithm 1.
Analysis and Discussions
As shown in Table 2, we choose MovieLens 1M as our ex-
perimental dataset, containing 6040 users and 3900 movies
totally, which is suitable for our experiments. To accelerate
our traing time, we pretrain our parameters first to find a
good parameter space.
Assume that V is the total vertexs, E is the total edges, I
is the iteration times, K is the average degree of the network
and D is the maximum dimension of the hidden layer. It is
not difficult to see that the complexity of the framework is
O(I(|V |KD + |E|)). In practical, we need to fine tune our
parameters acoording to diffrent datasets to get good results.
Experiments
Datasets
We choose two popular datasets, MovieLens and DBLP, in
our experiments. MovieLens 1 have been widely used in the
film recommendation system, where the weight of a edge
represents a user＊s rating of a movie. We select MovieLens
1M as the experimental data. DBLP 2 dataset contains the
data of the author＊s published articles, in which the weight
of a side indicates the number of papers published by a au-
thor in a venue. The statistics of the experimental dataset are
summarized in Table 2.
Table 2: Statistics of the experimental dataset, evaluation
metrics and parameters. AMD means adjacent matrix den-
sity.
Dataset MovieLens 1M DBLP
|U| 6040 6001
|V | 3900 1177
|E| 1000209 29256
Density 4.2% 0.4%
User AMD 85.4% 16.2%
Item AMD 63.1% 6.4%
Test Rate 0.4 0.4
Embedding Dimension 64 64
Metric Coverage, Novelty, F1, NDCG
In order to make the experimental data comparison more
reliable, we use the same training data and test data for all
1 htps://grouplens.org/datasets/movielens/
2 https://dblp.uni-trier.de/xml/
Algorithm 1 ANNER Framework
Input: The network G = (U,V,E); Weight matrix of the
network R; Embeddding dimension d;
Output: Recommendation list C i for each user u i ;
1: Construct the rating matrix R according to the network
G = (U,V,E);
2: Calculate the adjacent matrix R U and R V according to
the rating matrix R;
3: X = R U (X = R V );
4: Pretrain the model to obtain the initialized parameter
牟 = {牟 (1) ,...,牟 (K) };
5: repeat
6: Apply encoder to get
?
X and Y = Y
K
according to
X and 牟;
7: Calculate the loss according to the loss function
L ae x (X;牟) =
? ? X ? X
?
2
F
+ 2汐tr(Y
T LY ) + 汕L reg ;
8: Update parameters牟 through back-propagatethe en-
tir netwrk according to Eq. 12 and 13;
9: until converge
10: Obtain the embedding matrix U = Y
(K)
(V = Y
(K) );
11: for each edge(u i ,v j ) ﹋ E do
12: Update
? ↙
u i and
? ↙
v j using Eq. 10 and 11;
13: end for
14: Normalize the embedding matrix U and V with l2 norm;
15: Predict the ratings
?
R according to the embedding matrix
U and V;
16: Obtain the top-k recommendation list C i for each user
u i according to
?
R;
17: return C i ;
of the recommendation algorithms, then we set the scale of
test data to 0.4 and the embedding dimension to 64.
Baseline Algorithms
We choose four contrast algorithms based on neural
network(BiNE-Rec), matrix decomposition(SVD, NMF)
and traditional method(ItemKNN) respectively. The details
are as follows:
? ItemKNN(Pan, Dolog, and Xu 2012). The ItemKNN al-
gorithm find k items that are closest to the items to be
evaluated, and predicts the scores of the items to be eval-
uated according to the users＊ ratings on k items.
? SVD(Ariyoshi and Kamahara 2010). SVD is a matrix fac-
torization algorithm. SVD maps users and items to low-
dimensional vectors on potential factors by decomposing
scoring matrix. Then predict score according to the train-
ing model, which reduces the time complexity and space
complexity compared with ItemKNN.
? NMF(Luo et al. 2016). Non-negative Matrix Factoriza-
tion(NMF) is a latent factor model. The non-negative ma-
trix V is decomposed into two non-negative matrices W
and H, which is called the nonnegative matrix factoriza-
tion.
? BiNE(Gao et al. 2018) for recommendation(BiNE-Rec).
BiNE is a bipartite network representation learning al-
gorithm. It learns node representation by joint modeling
of explicit and high-order implicit relations. We use the
BiNE algorithm to learn the low-dimensional representa-
tion of users and items, so as to make recommendations.
Evaluation Metrics
We select four commonly used recommendation system
evaluation metrics: Coverage, Novelty, F1, and NDCG. The
analysis detail of four evaluation metrics are as follows.
? Coverage. Coverage score can evaluate the ability of dis-
covering long-tailed items. Simultaneously, coverage can
also reveal the diversity of the recommendation list. The
coverage metric is defined as follows:
Coverage =
? S u
i ﹋U
C i ?
|V |
(19)
? Novelty. The easiest way to measure novelty is to take
advantage of the average popularity of recommended re-
sults, as the less popular items are more likely to make
users exciting. The formula can be defined as follows:
Novelty =
1
|U|
|U|
X
i=1
k
X
j=1
pop j
k
(20)
pop j = log 2
|U|
d j
(21)
where k is the length of top-k recommendation list, and
d j is the degree of item node v j , which is the number of
links between item node v j and user set U.
? F1.F1scoreisacompromisebetweentheaccuracymetric
and the recall metric. The formula is defined as follows:
Precision =
TP
TP + FP
(22)
Recall =
TP
TP + FN
(23)
F1 =
2 ℅ Precision ℅ Recall
Precision + Recall
(24)
? NDCG. Since we evaluate the ordered top-k recom-
mendations, we choose a metric that is widely used
in ranking learning. Normalized Discounted Cumulative
Gain(NDCG) is defined as follows:
DCG@k =
k
X
i=1
2 rel i ? 1
log 2 (i + 1)
(25)
NDCG u i k =
DCG u i @k
IDCG u i
(26)
NDCG@k =
P
u i ﹋U NDCG u i @k
|U|
(27)
where rel i means the relevance of i-th item in recom-
mendationlist;IdealDiscountedCumulativeGain(IDCG)
means the best sorted recommendation list score.
In order to reflect the metric scores well, we repeat each
experiment 10 times, then take the average.
Table 3: Top-10 recommendation performance comparison of different algorithms on MovieLens and DBLP
Algorithm
MovieLens 1M DBLP
F1@10 NDCG@10 Nov@10 Cov@10 F1@10 NDCG@10 Nov@10 Cov@10
ItemKNN 2.63% 2.37% 4.37 14.82% 2.33% 2.42% 7.49 17.26%
NMF 0.54% 0.41% 6.44 17.05% 1.16% 1.01% 8.28 4.79%
SVD 2.96% 2.81% 3.72 17.77% 2.36% 2.92% 7.19 2.46%
BiNE-Rec 7.24% 7.08% 3.88 2.74% 11.32% 25.79% 5.19 1.93%
ANNER 6.96% 6.88% 7.48 47.66% 10.21% 20.76% 8.59 59.07%
Experiments Results
From the experimental results shown in Table 3 and Fig-
ure 7, we can see that deep neural network based algo-
rithms(BiNE and ANNER) performs better than traditional
methods(ItemKNN, NMF and SVD) in terms of F1 and
NDCG metric. We suppose that it is helpful to obtain the
first order and second order relation of the network through
deep learning methods. However, in terms of novelty and di-
versity, the BiNE algorithm performs not so well compared
withothersespeciallyonthemetricofcoverage.Wefindthat
the recommendation list generated by the BiNE algorithm is
roughly the same for each user, which is called Harry Potter
Problem. Harry Potter is a runaway bestseller, and every-
one likes it, which makes a tiny contribution for the system.
However, the vector of the popular item like Harry Potter
occupies a large weight in the embedding, leading to a list
of hot recommendation, and that makes the diversity score
almost zero.
By comparing the results of BiNE and traditional algo-
rithms(ItemKNN, NMF and SVD), we come to a conclusion
that accuracy and diversity of the prediction are two mutu-
ally exclusive metrics. However, the impact of diversity on
users can＊t be ignored. Through the comparison of ANNER
and BiNE, we have a significant increase in novelty and di-
versity while accuracy metric varies little from the optimal
value. We suppose that the main reasons are as follows: (1)
We mine the explicit and implicit relations of the network
throughthedeeplearningmethodbasedonauto-encoder.(2)
In order to weaken the influence of hot items on prediction,
we add regularization to the low-dimensional representation
of nodes. Hot items will get a big punishment in the pro-
cess of normalization, thus the less popular items could be
recommended first.
Conclusion
We propose an Auto-encoder based Normalized Net-
work Emedding method for Top-K Item Recommendation,
namely ANNER, to give a better recommendation. To fur-
ther address the problem of sparsity, we try to learn the net-
work embedding of adjacent matrix transformed from the
rating matrix, and then to preserve the network information
based on auto-encoder. To address the problem of diversity,
we propose to punish the network embedding with l2 norm.
Experiments on dataset MovieLens and DBLP show that our
method have a good performance on variety and accuracy.
Since the past decades have witnessed a big improvement
ofdeepneuralnetwork,wesupposetoexploitmoreinforma-
Figure 7: Performance comparison on two datasets. Each
figure describes the performance of five algorithms on four
metrics.
tion of the recommendation dataset, such as the timestamp
of each activity, the comment on each items, the information
of each items and so on. Furthermore, we are pretty inter-
ested in the audio analysis and video analysis of the dataset
if it is possible.
References
Ariyoshi, Y., and Kamahara, J. 2010. A hybrid recom-
mendation method with double svd reduction. In Interna-
tional Conference on Database Systems for Advanced Ap-
plications, 365每373.
Belkin, M., and Niyogi, P. 2014. Laplacian eigenmaps for
dimensionality reduction and data representation. Neural
Computation 15(6):1373每1396.
Brand, M. 2003. Fast online svd revisions for lightweight
recommender systems. In Siam International Conference on
Data Mining.
Cai, H.; Zheng, V. W.; and Chang, C. C. 2017. A compre-
hensive survey of graph embedding: Problems, techniques
and applications. IEEE Transactions on Knowledge & Data
Engineering.
Chen, C.; Zheng, X.; Wang, Y.; Hong, F.; and Lin, Z. 2014.
Context- ware collaborative topic regression with social ma-
trix factorization for recommender systems. In Twenty-
Eighth AAAI Conference on Artificial Intelligence, 9每15.
Collobert, R., and Weston, J. 2008. A unified architecture
for natural language processing:deep neural networks with
multitask learning. In International Conference on Machine
Learning, 160每167.
Cui, P.; Wang, X.; Pei, J.; and Zhu, W. 2017. A survey on
network embedding. IEEE Transactions on Knowledge &
Data Engineering PP(99):1每1.
Cunchao, T. U.; Yang, C.; Liu, Z.; and Sun, M. 2017. Net-
work representation learning: an overview. Scientia Sinica.
Dan, C.; Meier, U.; and Schmidhuber, J. 2012. Multi-
column deep neural networks for image classification sup-
plementary online material. Eprint Arxiv 157(10):3642每
3649.
Felfernig, A.; Jeran, M.; Ninaus, G.; Reinfrank, F.; and Re-
iterer, S. 2005. Toward the next generation of recommender
systems: Applications and research challenges. IEEE Trans-
actions on Knowledge & Data Engineering 17(6):734每749.
Gao, M.; Chen, L.; He, X.; and Zhou, A. 2018. Bine: Bipar-
tite network embedding. In The International ACM SIGIR
Conference, 715每724.
Goyal, P., and Ferrara, E. 2018. Graph embed-
ding techniques, applications, and performance: A survey.
Knowledge-Based Systems 151:78 每 94.
Graves, A.; Mohamed, A. R.; and Hinton, G. 2013. Speech
recognition with deep recurrent neural networks. In IEEE
International Conference on Acoustics, Speech and Signal
Processing, 6645每6649.
Hinton, G.; Deng, L.; Yu, D.; Dahl, G. E.; Mohamed,
A.; Jaitly, N.; Senior, A.; Vanhoucke, V.; Nguyen, P.; and
Sainath, T. N. 2012. Deep neural networks for acoustic
modelinginspeechrecognition:Thesharedviewsoffourre-
search groups. IEEE Signal Processing Magazine 29(6):82每
97.
Huang, Z.; Chung, W.; and Chen, H. 2014. A graph
model for e-commerce recommender systems. Journal of
the American Society for Information Science & Technology
55(3):259每274.
Li, X., and She, J. 2017. Collaborative variational autoen-
coder for recommender systems. In The ACM SIGKDD In-
ternational Conference, 305每314.
Luo, X.; Zhou, M.; Xia, Y.; and Zhu, Q. 2014. An efficient
non-negative matrix-factorization-based approach to collab-
orative filtering for recommender systems. IEEE Transac-
tions on Industrial Informatics 10(2):1273每1284.
Luo, X.; Zhou, M.; Li, S.; You, Z.; Xia, Y.; and Zhu, Q.
2016. A nonnegative latent factor model for large-scale
sparse matrices in recommender systems via alternating di-
rection method. IEEE Transactions on Neural Networks &
Learning Systems 27(3):579每592.
Mikolov, T.; Chen, K.; Corrado, G.; and Dean, J. 2013. Ef-
ficient estimation of word representations in vector space.
Computer Science.
Pan, R.; Dolog, P.; and Xu, G. 2012. Knn-based clustering
for improving social recommender systems. 7607:115每125.
Perozzi, B.; Al-Rfou, R.; and Skiena, S. 2014. Deep-
walk: online learning of social representations. In ACM
SIGKDD International Conference on Knowledge Discov-
ery and Data Mining, 701每710.
Schmidhuber, J. 2012. Multi-column deep neural networks
for image classification. In Computer Vision and Pattern
Recognition, 3642每3649.
Shen, Y., and Jin, R. 2012. Learning personal + social latent
factor model for social recommendation. In Proceedings of
the 18th ACM SIGKDD international conference on Knowl-
edge discovery and data mining, 1303每1311.
Tang, J.; Qu, M.; Wang, M.; Zhang, M.; Yan, J.; and Mei,
Q. 2015. Line:large-scale information network embedding.
1067每1077.
Wang, Z.; Zhang, J.; Feng, J.; and Chen, Z. 2014. Knowl-
edge graph embedding by translating on hyperplanes. In
Twenty-Eighth AAAI Conference on Artificial Intelligence,
1112每1119.
Wang, X.; Cui, P.; Wang, J.; Pei, J.; Zhu, W.; and Yang, S.
2017. Community preserving network embedding. In The
AAAI Conference on Artificial Intelligence.
Wang, D.; Cui, P.; and Zhu, W. 2016. Structural deep
network embedding. In ACM SIGKDD International Con-
ference on Knowledge Discovery and Data Mining, 1225每
1234.
Xu, L.; Ren, J. S. J.; Liu, C.; and Jia, J. 2014. Deep con-
volutional neural network for image deconvolution. In In-
ternational Conference on Neural Information Processing
Systems, 1790每1798.
Yang, C.; Zhao, D.; Zhao, D.; Chang, E. Y.; and Chang,
E. Y. 2015. Network representation learning with rich text
information. In International Conference on Artificial Intel-
ligence, 2111每2117.
Zhang, S.; Yao, L.; and Sun, A. 2017. Deep learning based
recommender system: A survey and new perspectives.
Zhu, H.; Li, X.; Zhang, P.; Li, G.; He, J.; Li, H.; and Gai,
K. 2018. Learning tree-based deep model for recommender
systems.
