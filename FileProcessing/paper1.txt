Auto-encoder based Normalized Network Embedding Methodfor Top-K Item RecommendationAbstractData sparsity and diversity of recommendation are two ma-jor problems in recommendation system. The way to fullyexploit the hidden features among users and items has at-tracted extensive research. The latent factor model (LFM)achieves good performance in Netflix Prize, which devotedto exploit latent factor. However, the problem of data sparsityin practical and expensive matrix decomposition operationsmake it difficult to deal with large scale networks. Networkrepresentation learning algorithm attracts much interest re-cently for ability to describe high-dimensional complex net-works as low-dimensional dense vectors, making it possibleto work with machine learning algorithm. Deep neural net-work has shown a powerful ability in capturing useful infor-mation of complex network. Therefore, we propose a methodto make top-k item recommendation through a normalizednetwork embedding method based on auto-encoder. We ana-lyzetheperformanceofseveraleffectivealgorithmsonDBLPand MovieLens1M datasets. Experimental results show thatthe proposed framework greatly improves the diversity andnovelty compared with the existing algorithms, while the F1score varies little.IntroductionRecommendation systems are widely used in industry. Wehave music applications, news applications and shoppingapplications in our mobile phone, which are supported byrecommendation systems with no doubt. However, there areproblems such as sparse data and lack of diversity in the rec-ommendation system. The study found that we do not makefull use of the hidden features of the data (Zhang, Yao, andSun 2017). Therefore, how to fully exploit the implicit fea-tures among users and items has become the main goal ofrecommendation system.The latent factor model (LFM) achieves good perfor-mance in Netflix Prize, which decomposes the rating matrixR into two low-dimensional matrices, trying to learn the im-plicit features of the data (Shen and Jin 2012). However, theproblem of data sparsity in practical and expensive matrixdecomposition operations make it difficult to deal with largescale networks.Network representation is the main representation for de-scribing the relationship between objects. The key is howCopyright c ? 2019, Association for the Advancement of ArtificialIntelligence (www.aaai.org). All rights reserved.to represent the features contained in the network effec-tively. Since the network representation learning algorithmcan describe high-dimensional complex networks as low-dimensional dense vectors, making it possible to combinecomplex tasks with machine learning algorithm effectively(Cui et al. 2017; Wang et al. 2017; 2014). In short, applynetwork representation learning algorithms to recommenda-tion systems plays an important role in academic value andapplication value (Cunchao et al. 2017). Simultaneously, thenetwork embedding method is more effective than matrixdecomposition in feature learning (Goyal and Ferrara 2018).In addition, the deep neural network has shown a revolu-tionary advances in image(Schmidhuber 2012; Dan, Meier,and Schmidhuber 2012; Xu et al. 2014), speech(Hinton etal. 2012; Graves, Mohamed, and Hinton 2013) and naturallanguage processing(Collobert and Weston 2008), which at-tracts researchers to apply it in capturing useful informationof complex network(Zhang, Yao, and Sun 2017). (Li andShe 2017) show that the auto-encoder performs robust onextracting the implicit relationships between items and userscollaboratively when processing the network representation.In order to solve the problem of data sparsity in ratingmatrix of recommendation system, we transform the ratingmatrix into user adjacency matrix and item adjacent matrix,which turns out to increase the data density effectively. Re-cent research on Taobao recommendation scenario showsthat the representation of complex networks by graph em-bedding can solve the problem of data sparse and cold startto a certain extent (Zhu et al. 2018). Above all, we proposeto choose graph embedding method with the help of auto-encoder to train our model.On the process of dealing with diversity and novelty, wefind that hot items usually have a high weight in the net-work, which leads to the large value of the final vector repre-sentation. As a result, the items recommended for each userare much the same (almost hot items only), which leaves anegative impact on users permanently even the accuracy andF1 metric of the experiment are good. Correspondingly, wefound that the hot goods can be punished to a certain extentthrough the normalized vector representation. Results showthat the prediction of the hot goods moves down in ranking,which makes ordinary items recommended in the top-k list,leading to a more diverse results.In short, the contributions of this paper are listed as fol-lows:? We propose an Auto-encoder based Normalized NetworkEmbedding method for Top-K Recommendation, namelyANNER, which combines the advantages of auto-encoderand network embedding algorithms.? We choose KL-divergence to extract explicit features andauto-encoder to extract the first-order and higher-orderproximity of implicit features so as to preserve the net-work structure more comprehensively.? We suggest to punish the popular items with the help ofL2 norm in the process of network embedding, whichturns out to be a good way to improve variety.? We have carried on the contrast experiments on the pop-ular datasets MovieLens and DBLP. Compared with theprevious effective recommendation algorithms, the diver-sity and novelty score have been greatly improved in ourmethod while the F1 score varies little.Related WorkRecommendation SystemsThe task of recommendation system is divided into rat-ing prediction and ranking prediction(Zhang, Yao, and Sun2017). Rating prediction task is to predict vacant scores inthe rating matrix(Chen et al. 2014). Ranking prediction gen-erates a list of items (top-k recommendation) that each useris most likely to consume. Recommendation system mod-els are mainly based on collaborative filtering, content-basedand hybrid recommendation systems(Felfernig et al. 2005).The early work is mainly based on collaborative filtering(CF) method to use the user＊s historical interactive informa-tion to recommend, including the traditional neighborhoodbased algorithm (ItemKNN, UserKNN)(Pan, Dolog, and Xu2012), latent factor based algorithm(NMF, SVD)(Luo et al.2014; 2016; Brand 2003) and graph based algorithm(Huang,Chung, and Chen 2014). Many recent efforts devoted touse deep learning models (such as CNN, RNN, RBM andAuto-encoder) to learn multi-layer representations of data toachieve better results(Zhang, Yao, and Sun 2017).Network EmbeddingAs an important form of data representation, graph appearsinawiderangeofreal-worldscenarios.Effectivegraphanal-ysis enables users to have a better understanding of the fea-tures behind the data. Network embedding maps graph datato a low dimensional space, which preserves the structure in-formation of graphs and the properties of graphs to the max-imum extent(Cui et al. 2017). Previous studies have shownthat network embedding has a wide range of applicationsin node classification, recommendation systems, link pre-diction, community discovery and visualization(Cai, Zheng,and Chang 2017).Network representation learning methods are mainlybased on matrix eigenvector computation, neural networkand matrix decomposition (Cunchao et al. 2017). Recentmethods are mainly based on random walk and deep neu-ral network(Cai, Zheng, and Chang 2017). Word2Vec im-proves the efficiency of word representation significantly bytransforming sparse discrete and high-dimensional vectorsintodensecontinuousandlow-dimensionalvectors(Mikolovet al. 2013). To make analogy with Word2Vec, the randomwalk based model (Perozzi, Al-Rfou, and Skiena 2014) gen-erates random paths on the network. By regarding a nodeas a word, we can treat a random path as a sentence, andthe neighborhood of a node can be identified by the co-occurrence rate in Word2Vec. At the same time, (Yang etal. 2015) proved that the DeepWalk algorithm is actuallyequivalent to the matrix decomposition of a particular ad-jacent matrix. Then, by learning the first and second orderrelations in the network, LINE further mine the informationof the network(Tang et al. 2015). SDNE starts to learn thehigher order relations of the network based on auto-encoder,which is supposed as to preserve the overall structure of thenetwork(Wang, Cui, and Zhu 2016), and so on. Recently,BiNE proposed a network embedding model for bipartitenetworks, which is the first work in bipartite network em-bedding(Gao et al. 2018).ANNER FrameworkThe goal of the recommendation system is to recommendthe customized item sequence to each user, so as to provide abetteruserexperiencetousersasfaraspossible.Thissectionwe will describe our framework in detail.Table 1: Notations and TermsSymbol DefinitionU = {u i } |U|i=1 ,V = {v j }|V |j=1user set, item setE ﹋ U ? V inter-set edgesG = (U,V,E) networkR = [r ij ] rating matrix? ↙u i ,? ↙v j embedding vectorsU = [ ? ↙ u i ], V = [ ? ↙ v j ] embedding matrixR U , R V adjacent matrix of U, VX = {x i } ni=1 ,?X = {? x i } ni=1input, reconstructedY(l)= {y (l)i} ni=1lth layer embeddingsW (l) ,?W (l) lth layer weightsb (l) , ? b (l) lth layer biases牟 = {W (l) ,?W (l) ,b (l) , ? b (l) } AE parametersProblem DefinitionProblem definition. The goal of the recommendation sys-tem is to recommend a list of items most likely to be con-sumed for each user, in a preference decrease order.Input. The network G = (U,V,E); Weight matrix of thenetwork R; Embeddding dimension d.Output. Predict a sequence of favorite items C i for eachuser u i .Notations. Notations and terms are listed in Table 1.FrameworkBased on the above analysis, in order to preserve the wholenetwork well, we propose a new framework Auto-encoderbased Normalized Network Embedding method for Top-KItem Recommendation (ANNER). With KL-divergence tocapture the explicit features and auto-encoder to mine theimplicit features of the network effectively, which can beobserved in Figure 1 and Figure 5.Figure 1: Flow Diagram of the ANNER Framework.Explicit RelationsAs shown in Figure 2, the edges between nodes u i and v jare defined as the explicit relationship in the recommenda-tion system scenario. Inspired by the first-order proximityin LINE (Tang et al. 2015) , we establish explicit relationsby considering the local proximity between two connectednodes. The joint probability between nodes u i and v j can bedefined as follows:P(i,j) =r ijPe ij ﹋E r ij(1)where r ij is the weight of edge e ij .Inspired by Word2Vec (Mikolov et al. 2013), we simulatethe interaction between two entities by inner product to es-timate the local proximity between two nodes in embeddingspace. The interactive value is transformed into probabilisticspace by sigmoid function.?P(i,j) =11 + exp(? ? ↙ u i T ? ↙ v j )(2)where? ↙u i and? ↙v j are the embedding vectors of nodes u iand v j respectively.With the empirical distribution of the co-occurrence prob-ability between nodes and the reconstructed distribution, wecan learn the embedding vector by minimizing the differ-ence. We choose the Kl-divergence as the difference mea-sure between their distributions. The loss function is definedas follows:minimize O KL = KL(P|| ? P)=≦ ?Xe ij ﹋Er ij log ? P(i,j)(3)By minimizing the loss function, two connected nodes inthe original network will be close to each other in the em-bedding space, thus maintaining local proximity.Figure 2: Explicit relations of the user feedback on items.Implicit RelationsWe define the implicit relationship as the relationship be-tween the same type nodes, that is, the relationship betweenthe users, or the relationship between the items. We can seefrom Figure 2 that there is an explicit relationship(user＊s rat-ing of item) between users and items in the recommendationsystem. However, the relationship between the users doesn＊texist explicitly. As shown in Figure 3, we can obtain the useradjacent matrix by calculating the similarity between users,items are the same.Figure 3: Adjacency Matrix calculated by the inner product.The left part is a rating matrix of 4 users and 6 items. Theright part are the user adjacent matrix and item adjacent ma-trix.In order to fully mine the local and global information ofadjacent matrices, and inspired by LINE (Tang et al. 2015),we model the first and second order relations of adjacentmatrices, as shown in Figure 4.Figure 4: Nodes 1 and 3 have a direct connection called first-order relations; nodes 1 and 2 have multiple common neigh-bors, which is called second-order relationsSince nodes 1 and 3 have a direct connection, they shouldbe placed closer in low-dimensional spaces, which is calledfirst-order relations; nodes 1 and 2 have multiple commonneighbors even they don＊t have an explicit connection, sothey should be placed closer in low-dimensional spaces,which is called second-order relations.Second Order Relations The idea of second-order rela-tions is that two nodes with common neighbors tend to becloser in embedding space, so the global structure of the net-work can be well preserved. Inspired by SDNE(Wang, Cui,and Zhu 2016), we can implement the auto-encoder algo-rithm with the idea of auto-encoder unsupervised learning.As shown in Figure 5, both the encoder and decoder partscontain multilayer nonlinear function, the encoder maps therawdatatothelow-dimensionalspace,andthedecodermap-ping the low-dimensional space representation to the recon-structed space. In this way, the network is more and more ro-bustbyminimizingthedifferencebetweentheoriginalspaceand the reconstructed space, so that the low-dimensionalrepresentation of each node can be obtained. For example,given x i the representation of each layer of the encoder is asfollows:y (1)i= 考(W (1) x i + b (1) ) (4)y (k)i= 考(W (k) y (k?1)i+ b (k) ),k = 2,...,K (5)where y (K)iis a low-dimensional representation, and thendecoder can perform an inverse operation to get the recon-structed ? x, so that the whole auto-coder can be trained byminimizing the loss functions:L =nXi=1k? x i ? x i k 22(6)On the one hand, the adjacency matrix is not a sparse ma-trix (The number of zero element in sparse matrix is muchlarger than the non-zero element, so we need to add moreweight to non-zero elements to make the training more ef-fective), which can be seen in Table 2. On the other hand, thesimilarity between users are different. So there is no need toappend a bias here. To have a better visualization of weightsin adjacency matrix, we have generated the distribution ofadjacency matrix weights in Figure 6. Since the long tail ef-fect exists, we only show the middle part for better analysis.First Order Relations Inspired by laplacian eigen-maps(LE) (Belkin and Niyogi 2014), we define the follow-ing loss functions to keep them close to each other in em-bedding space to preserve the local structure of the network.L 1st =nXi,j=1s i,j?y(K)i? y (K)j?22=nXi,j=1s i,j ky i ? y j k 22(7)We can see that by adding a penalty coefficient s ij to theloss function to make the related nodes closer in the lowdimensional space so as to preserve the local structure of thenetwork.Combining the first order relation and the second orderrelation, we define the joint optimal loss function as follows:L ae x = L 2nd + 汐L 1st + 汕L reg=? ? X ? X?2F+ 汐nXi,j=1s i,j ky i ? y j k 22+ 汕L reg(8)L reg =12KXk=1(?W(k) ?2F+? ? W(k) ?2F) (9)OptimizationFor the explicit relation, edge e ij ﹋ E, we use StochasticGradient Ascent(SGA) to update the embedding vectors? ↙u iand? ↙v j , so as to maximize KL-divergence O KL . The updatesto? ↙u i and? ↙v j are as follows:? ↙u i =? ↙u i + 竹{污w ij [1 ? 考( ? ↙ u i T ? ↙ v j )] ﹞? ↙v j } (10)? ↙v j =? ↙v j + 竹{污w ij [1 ? 考( ? ↙ u i T ? ↙ v j )] ﹞? ↙u i } (11)For implicit relations, we want to minimize loss functionL ae user and L ae item , take L ae user as an example, the pa-tial derivative is shown below:?L ae user? ? W (k)=?L 2nd? ? W (k)+?L reg? ? W (k)(12)?L ae user?W (k)=?L 2nd?W (k)+?L 1st?W (k)+?L reg?W (k)k = 1,...,K(13)Firstly,?L 2nd? ? W (K)can be rephrased as follows:?L 2nd? ? W (K)=?L 2nd? ? X﹞? ? X? ? W (K)(14)and?L 2nd? ? Xcan be rephrased as follows:?L 2nd? ? X= 2( ? X ? X) (15)For the second term, since?X = 考( ? Y(K?1) ?W (K) + ? b (K) ),so we can get?L 2nd? ? W (K) , and based on hack-propagation,we can iteratively obtain?L 2nd? ? W (k) ,k = 1,...,K ? 1 and?L 2nd?W (k) ,k = 1,...,K.Secondly, we continue to calculate the partial derivativeof?L 1st?W (k) . The loss function of L 1stcan be rephrased as fol-lows:L 1st =nXi,j=1s i,j ky i ? y j k 22= 2tr(YT LY )(16)whereL = D?S,D ﹋ R n?n isadiagonalmatrix,D i,i =Pj s i,j .Then the calculation of?L 1st?W (K)can be rephrased as fol-lows:?L 1st?W (K)=?L 1st?Y﹞?Y?W (K)(17)Figure 5: ANNER Framework, the implicit part include two auto-encoder to train on user adjacent matrix and item adjacentmatrixseperately,theexplicitpartfinetunetheembeddingbyKL-divergence,thenormalizepartaredesignedtofixthediversityproblem.Figure 6: Distributions of weights of adjacent matrix ofDBLP dataset.We can get?Y?W (K) , since Y = 考(Y(K?1) W (K)+ b (K) ).And?L 1st?Ycan be rephrased as follows:?L 1st?Y= 2(L + L T ) ﹞ Y (18)Similar to L 2nd , we can get the calculation of partialderivative of L 1st .Since we have calculated the partial derivatives of the pa-rameters, we can optimize our model using stochastic gradi-ent descent. Empirically, we pretrain our parameters first inorder to find a good region of parameter space. To sum up,our whole framework works as Algorithm 1.Analysis and DiscussionsAs shown in Table 2, we choose MovieLens 1M as our ex-perimental dataset, containing 6040 users and 3900 moviestotally, which is suitable for our experiments. To accelerateour traing time, we pretrain our parameters first to find agood parameter space.Assume that V is the total vertexs, E is the total edges, Iis the iteration times, K is the average degree of the networkand D is the maximum dimension of the hidden layer. It isnot difficult to see that the complexity of the framework isO(I(|V |KD + |E|)). In practical, we need to fine tune ourparameters acoording to diffrent datasets to get good results.ExperimentsDatasetsWe choose two popular datasets, MovieLens and DBLP, inour experiments. MovieLens 1 have been widely used in thefilm recommendation system, where the weight of a edgerepresents a user＊s rating of a movie. We select MovieLens1M as the experimental data. DBLP 2 dataset contains thedata of the author＊s published articles, in which the weightof a side indicates the number of papers published by a au-thor in a venue. The statistics of the experimental dataset aresummarized in Table 2.Table 2: Statistics of the experimental dataset, evaluationmetrics and parameters. AMD means adjacent matrix den-sity.Dataset MovieLens 1M DBLP|U| 6040 6001|V | 3900 1177|E| 1000209 29256Density 4.2% 0.4%User AMD 85.4% 16.2%Item AMD 63.1% 6.4%Test Rate 0.4 0.4Embedding Dimension 64 64Metric Coverage, Novelty, F1, NDCGIn order to make the experimental data comparison morereliable, we use the same training data and test data for all1 htps://grouplens.org/datasets/movielens/2 https://dblp.uni-trier.de/xml/Algorithm 1 ANNER FrameworkInput: The network G = (U,V,E); Weight matrix of thenetwork R; Embeddding dimension d;Output: Recommendation list C i for each user u i ;1: Construct the rating matrix R according to the networkG = (U,V,E);2: Calculate the adjacent matrix R U and R V according tothe rating matrix R;3: X = R U (X = R V );4: Pretrain the model to obtain the initialized parameter牟 = {牟 (1) ,...,牟 (K) };5: repeat6: Apply encoder to get?X and Y = YKaccording toX and 牟;7: Calculate the loss according to the loss functionL ae x (X;牟) =? ? X ? X?2F+ 2汐tr(YT LY ) + 汕L reg ;8: Update parameters牟 through back-propagatethe en-tir netwrk according to Eq. 12 and 13;9: until converge10: Obtain the embedding matrix U = Y(K)(V = Y(K) );11: for each edge(u i ,v j ) ﹋ E do12: Update? ↙u i and? ↙v j using Eq. 10 and 11;13: end for14: Normalize the embedding matrix U and V with l2 norm;15: Predict the ratings?R according to the embedding matrixU and V;16: Obtain the top-k recommendation list C i for each useru i according to?R;17: return C i ;of the recommendation algorithms, then we set the scale oftest data to 0.4 and the embedding dimension to 64.Baseline AlgorithmsWe choose four contrast algorithms based on neuralnetwork(BiNE-Rec), matrix decomposition(SVD, NMF)and traditional method(ItemKNN) respectively. The detailsare as follows:? ItemKNN(Pan, Dolog, and Xu 2012). The ItemKNN al-gorithm find k items that are closest to the items to beevaluated, and predicts the scores of the items to be eval-uated according to the users＊ ratings on k items.? SVD(Ariyoshi and Kamahara 2010). SVD is a matrix fac-torization algorithm. SVD maps users and items to low-dimensional vectors on potential factors by decomposingscoring matrix. Then predict score according to the train-ing model, which reduces the time complexity and spacecomplexity compared with ItemKNN.? NMF(Luo et al. 2016). Non-negative Matrix Factoriza-tion(NMF) is a latent factor model. The non-negative ma-trix V is decomposed into two non-negative matrices Wand H, which is called the nonnegative matrix factoriza-tion.? BiNE(Gao et al. 2018) for recommendation(BiNE-Rec).BiNE is a bipartite network representation learning al-gorithm. It learns node representation by joint modelingof explicit and high-order implicit relations. We use theBiNE algorithm to learn the low-dimensional representa-tion of users and items, so as to make recommendations.Evaluation MetricsWe select four commonly used recommendation systemevaluation metrics: Coverage, Novelty, F1, and NDCG. Theanalysis detail of four evaluation metrics are as follows.? Coverage. Coverage score can evaluate the ability of dis-covering long-tailed items. Simultaneously, coverage canalso reveal the diversity of the recommendation list. Thecoverage metric is defined as follows:Coverage =? S ui ﹋UC i ?|V |(19)? Novelty. The easiest way to measure novelty is to takeadvantage of the average popularity of recommended re-sults, as the less popular items are more likely to makeusers exciting. The formula can be defined as follows:Novelty =1|U||U|Xi=1kXj=1pop jk(20)pop j = log 2|U|d j(21)where k is the length of top-k recommendation list, andd j is the degree of item node v j , which is the number oflinks between item node v j and user set U.? F1.F1scoreisacompromisebetweentheaccuracymetricand the recall metric. The formula is defined as follows:Precision =TPTP + FP(22)Recall =TPTP + FN(23)F1 =2 ℅ Precision ℅ RecallPrecision + Recall(24)? NDCG. Since we evaluate the ordered top-k recom-mendations, we choose a metric that is widely usedin ranking learning. Normalized Discounted CumulativeGain(NDCG) is defined as follows:DCG@k =kXi=12 rel i ? 1log 2 (i + 1)(25)NDCG u i k =DCG u i @kIDCG u i(26)NDCG@k =Pu i ﹋U NDCG u i @k|U|(27)where rel i means the relevance of i-th item in recom-mendationlist;IdealDiscountedCumulativeGain(IDCG)means the best sorted recommendation list score.In order to reflect the metric scores well, we repeat eachexperiment 10 times, then take the average.Table 3: Top-10 recommendation performance comparison of different algorithms on MovieLens and DBLPAlgorithmMovieLens 1M DBLPF1@10 NDCG@10 Nov@10 Cov@10 F1@10 NDCG@10 Nov@10 Cov@10ItemKNN 2.63% 2.37% 4.37 14.82% 2.33% 2.42% 7.49 17.26%NMF 0.54% 0.41% 6.44 17.05% 1.16% 1.01% 8.28 4.79%SVD 2.96% 2.81% 3.72 17.77% 2.36% 2.92% 7.19 2.46%BiNE-Rec 7.24% 7.08% 3.88 2.74% 11.32% 25.79% 5.19 1.93%ANNER 6.96% 6.88% 7.48 47.66% 10.21% 20.76% 8.59 59.07%Experiments ResultsFrom the experimental results shown in Table 3 and Fig-ure 7, we can see that deep neural network based algo-rithms(BiNE and ANNER) performs better than traditionalmethods(ItemKNN, NMF and SVD) in terms of F1 andNDCG metric. We suppose that it is helpful to obtain thefirst order and second order relation of the network throughdeep learning methods. However, in terms of novelty and di-versity, the BiNE algorithm performs not so well comparedwithothersespeciallyonthemetricofcoverage.Wefindthatthe recommendation list generated by the BiNE algorithm isroughly the same for each user, which is called Harry PotterProblem. Harry Potter is a runaway bestseller, and every-one likes it, which makes a tiny contribution for the system.However, the vector of the popular item like Harry Potteroccupies a large weight in the embedding, leading to a listof hot recommendation, and that makes the diversity scorealmost zero.By comparing the results of BiNE and traditional algo-rithms(ItemKNN, NMF and SVD), we come to a conclusionthat accuracy and diversity of the prediction are two mutu-ally exclusive metrics. However, the impact of diversity onusers can＊t be ignored. Through the comparison of ANNERand BiNE, we have a significant increase in novelty and di-versity while accuracy metric varies little from the optimalvalue. We suppose that the main reasons are as follows: (1)We mine the explicit and implicit relations of the networkthroughthedeeplearningmethodbasedonauto-encoder.(2)In order to weaken the influence of hot items on prediction,we add regularization to the low-dimensional representationof nodes. Hot items will get a big punishment in the pro-cess of normalization, thus the less popular items could berecommended first.ConclusionWe propose an Auto-encoder based Normalized Net-work Emedding method for Top-K Item Recommendation,namely ANNER, to give a better recommendation. To fur-ther address the problem of sparsity, we try to learn the net-work embedding of adjacent matrix transformed from therating matrix, and then to preserve the network informationbased on auto-encoder. To address the problem of diversity,we propose to punish the network embedding with l2 norm.Experiments on dataset MovieLens and DBLP show that ourmethod have a good performance on variety and accuracy.Since the past decades have witnessed a big improvementofdeepneuralnetwork,wesupposetoexploitmoreinforma-Figure 7: Performance comparison on two datasets. Eachfigure describes the performance of five algorithms on fourmetrics.tion of the recommendation dataset, such as the timestampof each activity, the comment on each items, the informationof each items and so on. Furthermore, we are pretty inter-ested in the audio analysis and video analysis of the datasetif it is possible.ReferencesAriyoshi, Y., and Kamahara, J. 2010. A hybrid recom-mendation method with double svd reduction. In Interna-tional Conference on Database Systems for Advanced Ap-plications, 365每373.Belkin, M., and Niyogi, P. 2014. Laplacian eigenmaps fordimensionality reduction and data representation. NeuralComputation 15(6):1373每1396.Brand, M. 2003. Fast online svd revisions for lightweightrecommender systems. In Siam International Conference onData Mining.Cai, H.; Zheng, V. W.; and Chang, C. C. 2017. A compre-hensive survey of graph embedding: Problems, techniquesand applications. IEEE Transactions on Knowledge & DataEngineering.Chen, C.; Zheng, X.; Wang, Y.; Hong, F.; and Lin, Z. 2014.Context- ware collaborative topic regression with social ma-trix factorization for recommender systems. In Twenty-Eighth AAAI Conference on Artificial Intelligence, 9每15.Collobert, R., and Weston, J. 2008. A unified architecturefor natural language processing:deep neural networks withmultitask learning. In International Conference on MachineLearning, 160每167.Cui, P.; Wang, X.; Pei, J.; and Zhu, W. 2017. A survey onnetwork embedding. IEEE Transactions on Knowledge &Data Engineering PP(99):1每1.Cunchao, T. U.; Yang, C.; Liu, Z.; and Sun, M. 2017. Net-work representation learning: an overview. Scientia Sinica.Dan, C.; Meier, U.; and Schmidhuber, J. 2012. Multi-column deep neural networks for image classification sup-plementary online material. Eprint Arxiv 157(10):3642每3649.Felfernig, A.; Jeran, M.; Ninaus, G.; Reinfrank, F.; and Re-iterer, S. 2005. Toward the next generation of recommendersystems: Applications and research challenges. IEEE Trans-actions on Knowledge & Data Engineering 17(6):734每749.Gao, M.; Chen, L.; He, X.; and Zhou, A. 2018. Bine: Bipar-tite network embedding. In The International ACM SIGIRConference, 715每724.Goyal, P., and Ferrara, E. 2018. Graph embed-ding techniques, applications, and performance: A survey.Knowledge-Based Systems 151:78 每 94.Graves, A.; Mohamed, A. R.; and Hinton, G. 2013. Speechrecognition with deep recurrent neural networks. In IEEEInternational Conference on Acoustics, Speech and SignalProcessing, 6645每6649.Hinton, G.; Deng, L.; Yu, D.; Dahl, G. E.; Mohamed,A.; Jaitly, N.; Senior, A.; Vanhoucke, V.; Nguyen, P.; andSainath, T. N. 2012. Deep neural networks for acousticmodelinginspeechrecognition:Thesharedviewsoffourre-search groups. IEEE Signal Processing Magazine 29(6):82每97.Huang, Z.; Chung, W.; and Chen, H. 2014. A graphmodel for e-commerce recommender systems. Journal ofthe American Society for Information Science & Technology55(3):259每274.Li, X., and She, J. 2017. Collaborative variational autoen-coder for recommender systems. In The ACM SIGKDD In-ternational Conference, 305每314.Luo, X.; Zhou, M.; Xia, Y.; and Zhu, Q. 2014. An efficientnon-negative matrix-factorization-based approach to collab-orative filtering for recommender systems. IEEE Transac-tions on Industrial Informatics 10(2):1273每1284.Luo, X.; Zhou, M.; Li, S.; You, Z.; Xia, Y.; and Zhu, Q.2016. A nonnegative latent factor model for large-scalesparse matrices in recommender systems via alternating di-rection method. IEEE Transactions on Neural Networks &Learning Systems 27(3):579每592.Mikolov, T.; Chen, K.; Corrado, G.; and Dean, J. 2013. Ef-ficient estimation of word representations in vector space.Computer Science.Pan, R.; Dolog, P.; and Xu, G. 2012. Knn-based clusteringfor improving social recommender systems. 7607:115每125.Perozzi, B.; Al-Rfou, R.; and Skiena, S. 2014. Deep-walk: online learning of social representations. In ACMSIGKDD International Conference on Knowledge Discov-ery and Data Mining, 701每710.Schmidhuber, J. 2012. Multi-column deep neural networksfor image classification. In Computer Vision and PatternRecognition, 3642每3649.Shen, Y., and Jin, R. 2012. Learning personal + social latentfactor model for social recommendation. In Proceedings ofthe 18th ACM SIGKDD international conference on Knowl-edge discovery and data mining, 1303每1311.Tang, J.; Qu, M.; Wang, M.; Zhang, M.; Yan, J.; and Mei,Q. 2015. Line:large-scale information network embedding.1067每1077.Wang, Z.; Zhang, J.; Feng, J.; and Chen, Z. 2014. Knowl-edge graph embedding by translating on hyperplanes. InTwenty-Eighth AAAI Conference on Artificial Intelligence,1112每1119.Wang, X.; Cui, P.; Wang, J.; Pei, J.; Zhu, W.; and Yang, S.2017. Community preserving network embedding. In TheAAAI Conference on Artificial Intelligence.Wang, D.; Cui, P.; and Zhu, W. 2016. Structural deepnetwork embedding. In ACM SIGKDD International Con-ference on Knowledge Discovery and Data Mining, 1225每1234.Xu, L.; Ren, J. S. J.; Liu, C.; and Jia, J. 2014. Deep con-volutional neural network for image deconvolution. In In-ternational Conference on Neural Information ProcessingSystems, 1790每1798.Yang, C.; Zhao, D.; Zhao, D.; Chang, E. Y.; and Chang,E. Y. 2015. Network representation learning with rich textinformation. In International Conference on Artificial Intel-ligence, 2111每2117.Zhang, S.; Yao, L.; and Sun, A. 2017. Deep learning basedrecommender system: A survey and new perspectives.Zhu, H.; Li, X.; Zhang, P.; Li, G.; He, J.; Li, H.; and Gai,K. 2018. Learning tree-based deep model for recommendersystems.